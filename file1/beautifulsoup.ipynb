{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d77183-6beb-4fd0-9bd8-867e564baf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names: ['1. Ship of Theseus', '2. Iruvar', '3. Kaagaz Ke Phool', '4. Lagaan: Once Upon a Time in India', '5. Pather Panchali', '6. Charulata', '7. Rang De Basanti', '8. Dev.D', '9. 3 Idiots', '10. Awaara', '11. Nayakan', '12. Aparajito', '13. Pushpaka Vimana', '14. Pyaasa', '15. Ghatashraddha', '16. Sholay', '17. Aradhana', '18. Do Ankhen Barah Haath', '19. Bombay', '20. Neecha Nagar', '21. Do Bigha Zamin', '22. Garm Hava', '23. Piravi', '24. Mughal-E-Azam', '25. Amma Ariyan']\n",
      "Ratings: ['8.0', '8.4', '7.8', '8.1', '8.2', '8.1', '8.1', '7.9', '8.4', '7.8', '8.7', '8.2', '8.6', '8.3', '7.5', '8.1', '7.6', '8.4', '8.1', '6.6', '8.3', '8.0', '7.8', '8.1', '7.4']\n",
      "Years: ['2012', '1997', '1959', '2001', '1955', '1964', '2006', '2009', '2009', '1951', '1987', '1956', '1987', '1957', '1977', '1975', '1969', '1957', '1995', '1946', '1953', '1974', '1989', '1960', '1986']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the IMDb Top 100 Indian Movies list\n",
    "url = \"https://www.imdb.com/list/ls056092300/\"\n",
    "\n",
    "# Define headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Fetch the webpage\n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status()  \n",
    "\n",
    "# Parse the webpage content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Lists to hold the movie data\n",
    "names = []\n",
    "ratings = []\n",
    "years = []\n",
    "\n",
    "# Find all movie containers\n",
    "movie_containers = soup.find_all('div', class_=\"ipc-metadata-list-summary-item__tc\", limit=100)\n",
    "\n",
    "for movie in movie_containers:\n",
    "    # Get movie name\n",
    "    name_tag = movie.find('h3', class_=\"ipc-title__text\")\n",
    "    name = name_tag.text.strip() if name_tag else 'N/A'\n",
    "    names.append(name)\n",
    "\n",
    "    # Get movie rating\n",
    "    rating_tag = movie.find('span', class_=\"ipc-rating-star--rating\")\n",
    "    rating = rating_tag.text.strip() if rating_tag else 'N/A'\n",
    "    ratings.append(rating)\n",
    "\n",
    "    # Get movie release year\n",
    "    year_tag = movie.find('span', class_=\"sc-b189961a-8 hCbzGp dli-title-metadata-item\")\n",
    "    year = year_tag.text.strip() if year_tag else 'N/A'\n",
    "    years.append(year)\n",
    "\n",
    "# Print extracted data for verification\n",
    "print(\"Names:\", names)\n",
    "print(\"Ratings:\", ratings)\n",
    "print(\"Years:\", years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84782a2d-b86e-4e3d-a0de-fe10e1d52f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page fetched successfully\n",
      "Found 1 posts\n",
      "  Heading Date Content Likes YouTube Link\n",
      "0     N/A          N/A   N/A          N/A\n"
     ]
    }
   ],
   "source": [
    "# Define the URL of the Patreon page\n",
    "url = \"https://www.patreon.com/coreyms\"\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Send a request to the page\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Page fetched successfully\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the page, status code: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Parse the page content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Initialize lists to store post details\n",
    "headings = []\n",
    "dates = []\n",
    "contents = []\n",
    "likes = []\n",
    "youtube_links = []\n",
    "\n",
    "# Find all post containers\n",
    "posts = soup.find_all('div', {'data-tag': 'creator-public-page-recent-posts'})\n",
    "\n",
    "print(f\"Found {len(posts)} posts\")\n",
    "\n",
    "for post in posts:\n",
    "    try:\n",
    "        # Get post heading\n",
    "        heading = post.find('span', {'data-tag': 'post-title', 'class': 'sc-1cvoi1y-0 hxhWXn'})\n",
    "        headings.append(heading.text.strip() if heading else 'N/A')\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting heading: {e}\")\n",
    "        headings.append('N/A')\n",
    "    \n",
    "    try:\n",
    "        # Get post date\n",
    "        date = post.find('span')\n",
    "        dates.append(date.text.strip() if date else 'N/A')\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting date: {e}\")\n",
    "        dates.append('N/A')\n",
    "    \n",
    "    try:\n",
    "        # Get post content\n",
    "        content = post.find('div', {'id': 'cid-7'})\n",
    "        contents.append(content.text.strip() if content else 'N/A')\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting content: {e}\")\n",
    "        contents.append('N/A')\n",
    "    \n",
    "    try:\n",
    "        # Get post likes\n",
    "        like = post.find('span', {'data-tag': 'like-count', 'class': 'sc-iqseJM gEhAvT'})\n",
    "        likes.append(like.text.strip() if like else 'N/A')\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting likes: {e}\")\n",
    "        likes.append('N/A')\n",
    "    \n",
    "    try:\n",
    "        # Get YouTube link (if available)\n",
    "        youtube_link = 'N/A'\n",
    "        links = post.find_all('a')\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href and 'youtube.com' in href:\n",
    "                youtube_link = href\n",
    "                break\n",
    "        youtube_links.append(youtube_link)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting YouTube link: {e}\")\n",
    "        youtube_links.append('N/A')\n",
    "\n",
    "# Create a DataFrame to store the extracted data\n",
    "df = pd.DataFrame({\n",
    "    'Heading': headings,\n",
    "    'Date': dates,\n",
    "    'Content': contents,\n",
    "    'Likes': likes,\n",
    "    'YouTube Link': youtube_links\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('patreon_posts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f5a35c5-e14d-4a78-85f2-1482c7a091da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product 1:\n",
      "Name: bewakoof x dc\n",
      "Description: Men's Black Adam Graphic Printed T-shirt\n",
      "Price: N/A\n",
      "Image URL: N/A\n",
      "\n",
      "==================================================\n",
      "\n",
      "Product 2:\n",
      "Name: bewakoof x house of the dragon\n",
      "Description: Men's Black House Of The Dragon Iconic Graphic Printed T-shirt\n",
      "Price: N/A\n",
      "Image URL: N/A\n",
      "\n",
      "==================================================\n",
      "\n",
      "Product 3:\n",
      "Name: bewakoof x disney\n",
      "Description: Men's Black Mickey Trio Call (DL) Graphic Printed T-shirt\n",
      "Price: N/A\n",
      "Image URL: N/A\n",
      "\n",
      "==================================================\n",
      "\n",
      "Product 4:\n",
      "Name: Bewakoof®\n",
      "Description: Men's Black Warriors Graphic Printed Oversized T-shirt\n",
      "Price: N/A\n",
      "Image URL: N/A\n",
      "\n",
      "==================================================\n",
      "\n",
      "Product 5:\n",
      "Name: bewakoof x tom & jerry\n",
      "Description: Women's Blue Moody Jerry Graphic Printed Oversized T-shirt\n",
      "Price: N/A\n",
      "Image URL: N/A\n",
      "\n",
      "==================================================\n",
      "\n",
      "Product 6:\n",
      "Name: bewakoof x marvel\n",
      "Description: Men's Green Wakanda Forever Graphic Printed Oversized T-shirt\n",
      "Price: N/A\n",
      "Image URL: N/A\n",
      "\n",
      "==================================================\n",
      "\n",
      "Product 7:\n",
      "Name: Bewakoof®\n",
      "Description: Men's Sun-Kissed Green T-shirt\n",
      "Price: N/A\n",
      "Image URL: N/A\n",
      "\n",
      "==================================================\n",
      "\n",
      "Product 8:\n",
      "Name: Bewakoof®\n",
      "Description: Men's White Wander Geometry T-shirt\n",
      "Price: N/A\n",
      "Image URL: N/A\n",
      "\n",
      "==================================================\n",
      "\n",
      "Product 9:\n",
      "Name: Bewakoof®\n",
      "Description: Men's Black Guardian Wings Graphic Printed Oversized T-shirt\n",
      "Price: N/A\n",
      "Image URL: N/A\n",
      "\n",
      "==================================================\n",
      "\n",
      "Product 10:\n",
      "Name: Bewakoof®\n",
      "Description: Women's White & Purple Camo Printed Oversized T-shirt\n",
      "Price: N/A\n",
      "Image URL: N/A\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# URL of the Bewakoof bestseller page\n",
    "url = \"https://www.bewakoof.com/bestseller?sort=popular\"\n",
    "\n",
    "# Send a GET request to fetch the HTML content of the page\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    html_content = response.content\n",
    "else:\n",
    "    print(\"Failed to retrieve the page.\")\n",
    "    exit()\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "\n",
    "product_divs = soup.find_all('div', class_='productNaming bkf-ellipsis', limit=10)\n",
    "\n",
    "# Extract and display product details\n",
    "for i, div in enumerate(product_divs, start=1):\n",
    "    # Extract product name\n",
    "    name_tag = div.find('h3', class_='brand-name undefined')\n",
    "    name = name_tag.get_text(strip=True) if name_tag else 'N/A'\n",
    "    \n",
    "    # Extract product description\n",
    "    description_tag = div.find('h2', class_='clr-shade4 h3-p-name undefined false')\n",
    "    description = description_tag.get_text(strip=True) if description_tag else 'N/A'\n",
    "    \n",
    "    # Extract product price\n",
    "    price_tag = div.find('div',class_=\"actualPriceText clr-shade5 \")\n",
    "    price = price_tag.get_text(strip=True) if price_tag else 'N/A'\n",
    "    \n",
    "    # Extract image URL\n",
    "    img_tag = div.find('img',class_='productImgTag')\n",
    "    image_url = img_tag['src'] if img_tag and 'src' in img_tag.attrs else 'N/A'\n",
    "    \n",
    "    # Print the extracted details\n",
    "    print(f\"Product {i}:\")\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"Price: {price}\")\n",
    "    print(f\"Image URL: {image_url}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4433a-f16e-475b-a581-ed670ec1b0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f81d9e9-9a03-4a4d-9f2e-6378c8350b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: Yellen says U.S. economy remains solid, on path to 'soft landing' with no meaningful layoffs\n",
      "Date: 9 Hours Ago\n",
      "Link: https://www.cnbc.com/2024/09/07/yellen-says-us-economy-remains-solid-heading-toward-soft-landing.html\n",
      "----------------------------------------\n",
      "Headline: CIA director: There was risk of Russia using nuclear weapon early in Ukraine war\n",
      "Date: 13 Hours Ago\n",
      "Link: https://www.cnbc.com/2024/09/07/cia-director-russia-ukraine-war-nuclear-weapon-risk.html\n",
      "----------------------------------------\n",
      "Headline: Divorce parties reached an all-time high last year: 'It was incredibly therapeutic'\n",
      "Date: 17 Hours Ago\n",
      "Link: https://www.cnbc.com/2024/09/07/divorce-parties-reached-an-all-time-high-last-year-heres-why.html\n",
      "----------------------------------------\n",
      "Headline: \n",
      "Date: 18 Hours Ago\n",
      "Link: /investingclub/\n",
      "----------------------------------------\n",
      "Headline: This luxury psilocybin retreat 'creates better leaders,' founders say\n",
      "Date: 18 Hours Ago\n",
      "Link: https://www.cnbc.com/2024/09/07/this-luxury-psilocybin-retreat-creates-better-leaders-founders-say.html\n",
      "----------------------------------------\n",
      "Headline: Top 2 U.S. cities to retire are in Florida—No. 3 is nearly 1,800 miles away\n",
      "Date: 18 Hours Ago\n",
      "Link: https://www.cnbc.com/2024/09/07/wallethub-best-us-cities-to-retire.html\n",
      "----------------------------------------\n",
      "Headline: YouTube group The Try Guys quickly found success in launching subscription model\n",
      "Date: 18 Hours Ago\n",
      "Link: https://www.cnbc.com/2024/09/07/the-try-guys-has-quickly-found-success-in-launching-subscription-model.html\n",
      "----------------------------------------\n",
      "Headline: CEO quit his job, bought a snack company for $250K—now it brings in $103M/year\n",
      "Date: 19 Hours Ago\n",
      "Link: https://www.cnbc.com/2024/09/07/lesserevil-ceo-why-i-left-wall-street-for-a-failing-snack-company.html\n",
      "----------------------------------------\n",
      "Headline: 'The starving artist' is a myth, author says: How creatives sustain a career\n",
      "Date: 19 Hours Ago\n",
      "Link: https://www.cnbc.com/2024/09/07/stacey-derasmo-what-it-takes-for-creatives-to-sustain-a-career.html\n",
      "----------------------------------------\n",
      "Headline: Bill Gates wants to work another 20 to 30 years, like his friend Warren Buffett\n",
      "Date: 19 Hours Ago\n",
      "Link: https://www.cnbc.com/2024/09/07/bill-gates-retirement-sounds-awful-ill-work-like-warren-buffett.html\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# URL of the webpage to scrape\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Initialize lists to hold the scraped data\n",
    "headlines = []\n",
    "dates = []\n",
    "links = []\n",
    "\n",
    "# Scrape the headlines, dates, and links\n",
    "articles = soup.find_all('div', class_='LatestNews-headlineWrapper')\n",
    "for article in articles[:10]: \n",
    "   \n",
    "    headline = article.find('a').text.strip()\n",
    "    headlines.append(headline)\n",
    "    \n",
    "  \n",
    "    link = article.find('a')['href']\n",
    "    links.append(link)\n",
    "    \n",
    "   \n",
    "    date = article.find('time').text.strip() if article.find('time') else 'N/A'\n",
    "    dates.append(date)\n",
    "\n",
    "# Print the results\n",
    "for i in range(len(headlines)):\n",
    "    print(f\"Headline: {headlines[i]}\")\n",
    "    print(f\"Date: {dates[i]}\")\n",
    "    print(f\"Link: {links[i]}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb3941b9-7de2-4f9a-824c-7e617481e629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Implementation of artificial intelligence in agriculture for optimisation of irrigation and application of pesticides and herbicides\n",
      "Date: 2020\n",
      "Authors: Tanha Talaviya |  Dhara Shah |  Nivedita Patel |  Hiteshri Yagnik |  Manan Shah\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/\"\n",
    "\n",
    "# Send a GET request to fetch the HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all article containers\n",
    "    articles = soup.find_all('section', class_=\"panelled-container mb-xl-0\")\n",
    "\n",
    "    # Loop through each article and extract details\n",
    "    for article in articles:\n",
    "        # Extract title\n",
    "        title_tag = article.find('h2', class_=\"h5 article-title\")\n",
    "        title = title_tag.text.strip() if title_tag else 'N/A'\n",
    "\n",
    "        # Extract date\n",
    "        date_tag = article.find('p', class_=\"article-date\") \n",
    "        date = date_tag.text.strip() if date_tag else 'N/A'\n",
    "\n",
    "        # Extract authors\n",
    "        author_tag = article.find('p', class_=\"article-authors\") \n",
    "        authors = author_tag.text.strip() if author_tag else 'N/A'\n",
    "\n",
    "        # Print extracted information\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Date: {date}\")\n",
    "        print(f\"Authors: {authors}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc583e-4c82-4e6b-be0e-85f163d0c384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
